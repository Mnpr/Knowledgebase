# Dimensionality Reduction

**Given :** Collection of N high-dimensional objects $x_1, ... x_n.$

- **Exploration** of the arrangement of the high-dimensional objects in data space.
  
  - Data Visualization.
  
  - Structure of the data( e.g. Clusters, Local Structures, ... )

- **Reduction** of High dimensional objects by building a *map* in which distances between points reflect similarities in data.
  
  - Minimize some objective function that measures the *discrepancy* between similarities in the data and similarities in the map.( i.e. reflect the data in low dimension preserving similarities/characteristics. )
  
  - Termed as Embedding or Reduction or Multidimensional Scaling.

**Typical Techniques**

- Matrix Factorizaton
  
  - e.g. PCA, Linear Autoencoder, Word2Vec, Latent Dirichlet Allocation e.t.c.

- Neighbor Graphs
  
  - e.g. ISOMAP, t-SNE, LLE, UMAP, Laplacian Eigenmaps, LTSA e.t.c.

## Methods

- Feature Selection
  
  - Choosing improtant features and ignoring the rest

- Feature Transformation/ Projection
  
  - Project the original $d$ dimensions to new $k\lt d$ dimensions( transformation )
  
  - Principal component analysis( PCA )
  
  - Linear discriminant analysis( LDA )

- Clustering

### Feature Transformation

Finding linear transformations applied to the original data $x_1, ...,x_N \in \mathbb{R}^d$

**LDA** : 

- Applied to *labelled data ( Supervised )*

- Finding a low-dimensional space such that when $x$ is projected, class are well separated

**PCA**

- Applied to *unlabelled data ( Unsupervised )*

- Finding a low-dimensional space such that when $x$ is projected, informaiton loss is minimized.

- Principal components : direction in the data with largest variation

@ FH Lecture

## LDA( Linear Discriminant Analysis )

## PCA( Principal Component Analysis )

Dimensionality reduction technique that finds the variance maximizing directions onto which to project the data.

**Eigenvalue, eigenvector** - Given a matrix $A\in\mathbb{R}^{n\times n}$ , $\lambda$ is said to be and eigenvalue of $A$ if there exists a vector $z \in\mathbb{R}^n\backslash\{0\}$, called eigenvector, such that we have $Az=\lambda z$

**Spectral theorem** ― Let $A\in\mathbb{R}^{n\times n}$. If $A$ is symmetric, then $A$ is diagonalizable by a real orthogonal matrix $U\in\mathbb{R}^{n\times n}$. By noting $\Lambda=\textrm{diag}(\lambda_1,...,\lambda_n)$, we have:

$$
\boxed{\exists\Lambda\textrm{ diagonal},\quad A=U\Lambda U^T}
$$

*Remark: the eigenvector associated with the largest eigenvalue is called principal eigenvector of matrix A.*

**Algorithm** ― The Principal Component Analysis (PCA) procedure is a dimension reduction technique that projects the data on $k$

dimensions by maximizing the variance of the data as follows:

- <u>Step 1</u>: Normalize the data to have a mean of 0 and standard deviation of 1.

$$
\boxed{x_j^{(i)}\leftarrow\frac{x_j^{(i)}-\mu_j}{\sigma_j}}



{where}\quad\boxed{\mu_j = \frac{1}{m}\sum_{i=1}^mx_j^{(i)}} 
$$

$$
{and}\quad\boxed{\sigma_j^2=\frac{1}{m}\sum_{i=1}^m(x_j^{(i)}-\mu_j)^2}
$$

- <u>Step 2:</u>  Compute $\displaystyle\Sigma=\frac{1}{m}\sum_{i=1}^mx^{(i)}{x^{(i)}}^T\in\mathbb{R}^{n\times n}$ Which is symmetric with eigenvalues.

- <u>Step 3:</u> Compute $u_1,..., u_k \in\mathbb{R}^n$ the orthogonal principal eigenvectors of $\Sigma$, i.e. the orthogonal eigenvectors of the $k$ largest eigenvalues.

- <u>Step 4:</u> Project the data on $\textrm{span}_\mathbb{R}(u_1,...,u_k)$.

This procedure maximizes the variance among all $k$-dimensional spaces.

![](../../ImgResources/pca.png)

@ [Shervine Amidi](https://stanford.edu/~shervine)

## IDA( Independent Component analysis )

It is  a technique to find the underlying generating sources.

**Assumption :** Assuming out data $x$ has been generated by n-dimensional source vector $s=(s_1,...,s_n)$, where $s_i$ are independent random variables, via a mixing and non-singular matrix $A$ as follows:

$$
\boxed{x=As}
$$

The goal is to find the unmixing matrix $W=A^{-1}$

**Bell and Sejnowski ICA algorithm** ― This algorithm finds the unmixing matrix $W$ by following the steps below:

- Write the probability of $x=As=W^{-1}s$ as :

$$
p(x)=\prod_{i=1}^np_s(w_i^Tx)\cdot|W|
$$

- Write the log likelihood given our training data $\{x^{(i)}, i\in[\![1,m]\!]\}$
  and by noting $g$ the sigmoid function as:

$$
l(W)=\sum_{i=1}^m\left(\sum_{j=1}^n\log\Big(g'(w_j^Tx^{(i)})\Big)+\log|W|\right)
$$

Therefore, the stochastic gradient ascent learning rule is such that for each training example $x^{(i)}$, we update $W$ as follows :

$$
\boxed{W\longleftarrow W+\alpha\left(\begin{pmatrix}1-2g(w_1^Tx^{(i)})\\1-2g(w_2^Tx^{(i)})\\\vdots\\1-2g(w_n^Tx^{(i)})\end{pmatrix}{x^{(i)}}^T+(W^T)^{-1}\right)}
$$

@ [Shervine Amidi](https://stanford.edu/~shervine)

****

## TODO:

- [ ] LDA

- [ ] t-SNE

- [ ] 
